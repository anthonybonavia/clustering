{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da54281-62a3-47b1-8bc9-651bc685cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom colormap for visualization\n",
    "colors = [(1, 0, 0), (0, 0, 1)]  # Red to Blue\n",
    "cmap_name = \"red_blue\"\n",
    "red_blue_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "\n",
    "# Data preparation and clustering functions (load_data, impute_sofa_day3, prepare_data, perform_kmeans, etc.)\n",
    "# Data preparation functions\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def impute_sofa_day3(row):\n",
    "    if pd.isna(row['SOFA_D3']):\n",
    "        if row['mortality'] == 1 and (row['dud'] == 2 or row['dud'] == 3):\n",
    "            return 24\n",
    "        else:\n",
    "            return np.nan\n",
    "    return row['SOFA_D3']\n",
    "\n",
    "def prepare_data(df, columns_of_interest):\n",
    "    df['SOFA_D3'] = df.apply(impute_sofa_day3, axis=1)\n",
    "    df.dropna(subset=['SOFA_D3'], inplace=True)\n",
    "    df[columns_of_interest] = np.log1p(df[columns_of_interest])\n",
    "    return df\n",
    "\n",
    "# Clustering Functions\n",
    "def perform_kmeans(X_scaled, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    return labels\n",
    "\n",
    "def all_subsets(columns):\n",
    "    \"\"\"Generate all non-empty combinations of the column list.\"\"\"\n",
    "    return chain(*[combinations(columns, i + 1) for i in range(len(columns))])\n",
    "\n",
    "def perform_agglomerative(X_scaled, num_clusters):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "    labels = agglomerative.fit_predict(X_scaled)\n",
    "    return labels\n",
    "\n",
    "def perform_spectral(X_scaled, num_clusters):\n",
    "    spectral = SpectralClustering(n_clusters=num_clusters, random_state=42)\n",
    "    labels = spectral.fit_predict(X_scaled)\n",
    "    return labels\n",
    "\n",
    "def evaluate_clustering_with_labels(X_scaled, labels):\n",
    "    # Check for noise labels produced by DBSCAN and handle them\n",
    "    if -1 in labels:\n",
    "        # Filter out noise points for metric calculation\n",
    "        core_samples_mask = labels != -1\n",
    "        X_scaled_core = X_scaled[core_samples_mask]\n",
    "        labels_core = labels[core_samples_mask]\n",
    "        if len(np.unique(labels_core)) <= 1:\n",
    "            return -1, float('inf')  # Invalid scenario\n",
    "    else:\n",
    "        X_scaled_core = X_scaled\n",
    "        labels_core = labels\n",
    "        \n",
    "    silhouette = silhouette_score(X_scaled_core, labels_core) if len(set(labels_core)) > 1 else -1\n",
    "    davies_bouldin = davies_bouldin_score(X_scaled_core, labels_core) if len(set(labels_core)) > 1 else float('inf')\n",
    "    return silhouette, davies_bouldin\n",
    "\n",
    "# Function to iterate over all combinations of columns and evaluate clustering\n",
    "def cluster_and_evaluate_all_combinations(df_subset, columns_of_interest, methods):\n",
    "    best_overall_metrics = {'Silhouette': -1, 'Davies_Bouldin': float('inf'), 'Composite': -float('inf'),\n",
    "                            'Columns': None, 'Method': None, 'Num_Clusters': None}\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for num_features in range(1, len(columns_of_interest) + 1):\n",
    "        for cols in combinations(columns_of_interest, num_features):\n",
    "            df_imputed = imputer.fit_transform(df_subset[list(cols)])\n",
    "            X_scaled = scaler.fit_transform(df_imputed)\n",
    "\n",
    "            for method in methods:\n",
    "                for num_clusters in range(2, 11):\n",
    "                    labels = None\n",
    "                    if method == 'K-means':\n",
    "                        labels = perform_kmeans(X_scaled, num_clusters)\n",
    "                    elif method == 'Agglomerative':\n",
    "                        labels = perform_agglomerative(X_scaled, num_clusters)\n",
    "                    elif method == 'Spectral':\n",
    "                        labels = perform_spectral(X_scaled, num_clusters)\n",
    "\n",
    "                    silhouette, davies_bouldin = evaluate_clustering_with_labels(X_scaled, labels)\n",
    "                    composite = silhouette - davies_bouldin\n",
    "\n",
    "                    if composite > best_overall_metrics['Composite']:\n",
    "                        best_overall_metrics.update({\n",
    "                            'Silhouette': silhouette,\n",
    "                            'Davies_Bouldin': davies_bouldin,\n",
    "                            'Composite': composite,\n",
    "                            'Columns': cols,\n",
    "                            'Method': method,\n",
    "                            'Num_Clusters': num_clusters\n",
    "                        })\n",
    "\n",
    "    return best_overall_metrics\n",
    "\n",
    "def cluster_and_evaluate_all_combinations(df_subset, columns_of_interest, methods, imputer, scaler):\n",
    "    best_overall_metrics = {'Silhouette': -1, 'Davies_Bouldin': float('inf'), 'Composite': -float('inf'),\n",
    "                            'Columns': None, 'Method': None, 'Num_Clusters': None}\n",
    "\n",
    "    for num_features in range(1, len(columns_of_interest) + 1):\n",
    "        for cols in combinations(columns_of_interest, num_features):\n",
    "            df_imputed = imputer.fit_transform(df_subset[list(cols)])\n",
    "            X_scaled = scaler.fit_transform(df_imputed)\n",
    "                      \n",
    "            for method in methods:\n",
    "                for num_clusters in range(2, 11):\n",
    "                    labels = None\n",
    "                    if method == 'K-means':\n",
    "                        labels = perform_kmeans(X_scaled, num_clusters)\n",
    "                    elif method == 'Agglomerative':\n",
    "                        labels = perform_agglomerative(X_scaled, num_clusters)\n",
    "                    elif method == 'Spectral':\n",
    "                        labels = perform_spectral(X_scaled, num_clusters)\n",
    "\n",
    "                    silhouette, davies_bouldin = evaluate_clustering_with_labels(X_scaled, labels)\n",
    "                    composite = silhouette - davies_bouldin\n",
    "\n",
    "                    if composite > best_overall_metrics['Composite']:\n",
    "                        best_overall_metrics.update({\n",
    "                            'Silhouette': silhouette,\n",
    "                            'Davies_Bouldin': davies_bouldin,\n",
    "                            'Composite': composite,\n",
    "                            'Columns': cols,\n",
    "                            'Method': method,\n",
    "                            'Num_Clusters': num_clusters\n",
    "                        })\n",
    "\n",
    "    return best_overall_metrics\n",
    "   \n",
    "def cluster_and_evaluate(df_subset, columns_of_interest, methods):\n",
    "    best_metrics_by_method = {}\n",
    "    best_labels_by_method = {}\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    scaler = StandardScaler()\n",
    "    df_imputed = imputer.fit_transform(df_subset[columns_of_interest])\n",
    "    X_scaled = scaler.fit_transform(df_imputed)\n",
    "    \n",
    "    for method in methods:\n",
    "        best_silhouette = -1\n",
    "        best_davies_bouldin = float('inf')\n",
    "        best_labels = None\n",
    "        best_num_clusters = 0\n",
    "\n",
    "        for num_clusters in range(2, 11):\n",
    "            if method == 'K-means':\n",
    "                labels = perform_kmeans(X_scaled, num_clusters)\n",
    "            elif method == 'Agglomerative':\n",
    "                labels = perform_agglomerative(X_scaled, num_clusters)\n",
    "            elif method == 'Spectral':\n",
    "                labels = perform_spectral(X_scaled, num_clusters)            \n",
    "\n",
    "            silhouette, davies_bouldin = evaluate_clustering_with_labels(X_scaled, labels)\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_labels = labels\n",
    "                best_num_clusters = num_clusters\n",
    "\n",
    "        composite = best_silhouette - davies_bouldin\n",
    "        if best_labels is not None:\n",
    "            best_metrics_by_method[method] = {'Silhouette': best_silhouette, 'Davies_Bouldin': davies_bouldin, 'Composite': composite, 'Num_Clusters': best_num_clusters}\n",
    "            best_labels_by_method[method] = best_labels\n",
    "\n",
    "    return best_metrics_by_method, best_labels_by_method\n",
    "\n",
    "# Function for plotting scatter plot\n",
    "def plot_and_save_scatter(df, method, column, labels, title, filename):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    unique_labels = np.unique(labels)\n",
    "    palette = sns.color_palette(\"pastel\", n_colors=len(unique_labels))  # More conservative color palette\n",
    "    sns.scatterplot(x=df[column], y=df['SOFA_D3'], hue=labels, palette=palette)\n",
    "    \n",
    "    mean_values = df.groupby(labels)['SOFA_D3'].mean().to_dict()\n",
    "    legend_labels = [f\"Cluster {c}: mean SOFA_48h = {mean_values[c]:.2f}\" for c in unique_labels]\n",
    "    # Create custom handles for the legend\n",
    "    handles = [mpatches.Patch(color=palette[i], label=legend_labels[i]) for i in range(len(unique_labels))]\n",
    "    plt.legend(title='Cluster ID', handles=handles)\n",
    "\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('SOFA_48h')\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_sofa_distribution_by_cluster_and_subset(df, labels, filename):\n",
    "    df['Cluster'] = labels\n",
    "    df['Sepsis'] = df['Sepsis'].replace({0: 'Non-Sepsis Patients', 1: 'Sepsis Patients'})\n",
    "\n",
    "    g = sns.FacetGrid(df, col=\"Sepsis\", row=\"Cluster\", margin_titles=True, height=3)\n",
    "    g.map(sns.kdeplot, \"SOFA_D3\", fill=True)\n",
    "\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"Cluster {row_name}\")\n",
    "    g.set_axis_labels(\"SOFA_48h\", \"Density\")\n",
    "    for ax in g.axes.flat:\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "    # Removing the super title to avoid overlaying\n",
    "    g.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data('20240104_Data_combined_python_D1.csv')\n",
    "    df = prepare_data(df, columns_of_interest)\n",
    "    methods = ['K-means', 'Agglomerative', 'Spectral']\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for subset_name, df_subset in [(\"Septic\", df[df['Sepsis'] == 1]), (\"Non-Septic\", df[df['Sepsis'] == 0]), (\"All Patients\", df)]:\n",
    "        best_metrics = cluster_and_evaluate_all_combinations(df_subset, columns_of_interest, methods, imputer, scaler)\n",
    "        print(f\"Best Metrics for {subset_name}: {best_metrics}\")\n",
    "        \n",
    "        method = best_metrics['Method']\n",
    "        num_clusters = best_metrics['Num_Clusters']\n",
    "        best_cols = best_metrics['Columns']\n",
    "        df_imputed = imputer.fit_transform(df_subset[list(best_cols)])\n",
    "        X_scaled = scaler.fit_transform(df_imputed)\n",
    "\n",
    "        labels = None\n",
    "        if method == 'K-means':\n",
    "            labels = perform_kmeans(X_scaled, num_clusters)\n",
    "        elif method == 'Agglomerative':\n",
    "            labels = perform_agglomerative(X_scaled, num_clusters)\n",
    "        elif method == 'Spectral':\n",
    "            labels = perform_spectral(X_scaled, num_clusters)\n",
    "\n",
    "        plot_column = best_cols[0]\n",
    "        plot_and_save_scatter(df_subset, method, plot_column, labels, f\"{subset_name} Scatter Plot - {plot_column} vs SOFA_48h\", f\"{subset_name}_scatter_plot_{plot_column}.png\")\n",
    "        plot_sofa_distribution_by_cluster_and_subset(df_subset, labels, f\"{subset_name}_SOFA_D3_distribution.png\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data('20240104_Data_combined_python_D1.csv')\n",
    "    df = prepare_data(df, columns_of_interest)\n",
    "    methods = ['K-means', 'Agglomerative', 'Spectral']\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for subset_name, df_subset in [(\"Septic\", df[df['Sepsis'] == 1]), (\"Non-Septic\", df[df['Sepsis'] == 0]), (\"All Patients\", df)]:\n",
    "        print(f\"----- {subset_name} -----\")\n",
    "        for method in methods:\n",
    "            best_metrics_by_method, best_labels_by_method = cluster_and_evaluate(df_subset, columns_of_interest, [method])\n",
    "            \n",
    "            best_metrics = best_metrics_by_method[method]\n",
    "            print(f\"Method: {method}\")\n",
    "            print(f\"Silhouette: {best_metrics['Silhouette']}\")\n",
    "            print(f\"Davies-Bouldin: {best_metrics['Davies_Bouldin']}\")\n",
    "            print(f\"Best Columns: {best_metrics['Columns']}\")\n",
    "            print(f\"Number of Clusters: {best_metrics['Num_Clusters']}\\n\")\n",
    "\n",
    "            method = best_metrics['Method']\n",
    "            num_clusters = best_metrics['Num_Clusters']\n",
    "            best_cols = best_metrics['Columns']\n",
    "            df_imputed = imputer.fit_transform(df_subset[list(best_cols)])\n",
    "            X_scaled = scaler.fit_transform(df_imputed)\n",
    "\n",
    "            labels = None\n",
    "            if method == 'K-means':\n",
    "                labels = perform_kmeans(X_scaled, num_clusters)\n",
    "            elif method == 'Agglomerative':\n",
    "                labels = perform_agglomerative(X_scaled, num_clusters)\n",
    "            elif method == 'Spectral':\n",
    "                labels = perform_spectral(X_scaled, num_clusters)\n",
    "\n",
    "            plot_column = best_cols[0]\n",
    "            plot_and_save_scatter(df_subset, method, plot_column, labels, f\"{subset_name} Scatter Plot - {plot_column} vs SOFA_48h\", f\"{subset_name}_scatter_plot_{plot_column}.png\")\n",
    "            plot_sofa_distribution_by_cluster_and_subset(df_subset, labels, f\"{subset_name}_SOFA_D3_distribution.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
